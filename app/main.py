from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, Dict, List
import traceback
import os
from openai import OpenAI

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
ADVICE_MODEL = os.getenv("ADVICE_MODEL", "gpt-4.1-mini")


# â”€â”€ ëª¨ë¸ ì„¤ì •
MODEL_NAME = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
VERSION = "v0.3"
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)
model.eval()

app = FastAPI(title="Growlog AI Sentiment & Quantum API", version=VERSION)

# â”€â”€ ì…ë ¥/ì¶œë ¥ ëª¨ë¸ ì •ì˜
class AdviceRequest(BaseModel):
    text: str           # ì˜¤ëŠ˜ ê°ì • ë©”ëª¨ or íšŒê³  ë‚´ìš©
    emoji: str | None = None  # ì„ íƒ: ê°ì • ì´ëª¨ì§€

class TextInput(BaseModel):
    text: str

class ProbInput(BaseModel):
    positive: float
    neutral: float
    negative: float

class PhaseInput(BaseModel):
    positive: float = 0.0
    neutral: float = 0.0
    negative: float = 0.0

class OmegaInput(BaseModel):
    positive: float = 1.0
    neutral: float = 1.3
    negative: float = 1.7

class QuantumSimRequest(BaseModel):
    text: Optional[str] = None
    probs: Optional[ProbInput] = None
    duration: float = 30.0
    dt: float = 0.05
    coherence: float = Field(0.75, ge=0.0, le=1.0)
    phases: PhaseInput = PhaseInput()
    omegas: OmegaInput = OmegaInput()

class ComponentOut(BaseModel):
    label: str
    probability: float
    amplitude: float
    phase: float
    omega: float

class QuantumSimResponse(BaseModel):
    time: List[float]
    I_total: List[float]
    components: List[ComponentOut]
    summary: Dict[str, float]

def build_fallback_advice(text: str, emoji: str | None = None) -> str:
    """OpenAI í˜¸ì¶œì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¡°ì–¸."""
    t = text.strip()

    negative_words = ["í”¼ê³¤", "í˜ë“¤", "ìš°ìš¸", "ë¶ˆì•ˆ", "ê±±ì •", "ì§œì¦", "ì§€ì³¤", "ë²„ê²", "ë¬´ê¸°ë ¥"]
    stress_words = ["ê³¼ì œ", "ìˆ™ì œ", "ì‹œí—˜", "ê³µë¶€", "ì¼ì´ ë§", "ë§ˆê°", "ë°ë“œë¼ì¸"]
    body_words = ["ë‘í†µ", "ë¨¸ë¦¬ ì•„í”„", "ì–´ì§€ëŸ½", "ëª¸ì‚´", "ê°ê¸°"]

    is_negative = any(w in t for w in negative_words)
    is_stress   = any(w in t for w in stress_words)
    is_body     = any(w in t for w in body_words)

    bad_emojis = ["ğŸ˜¢", "ğŸ˜­", "ğŸ˜", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜¤", "ğŸ˜«", "ğŸ˜©", "ğŸ˜´", "ğŸ¥²"]
    good_emojis = ["ğŸ˜„", "ğŸ™‚", "ğŸ¤©", "ğŸ˜Š", "ğŸ˜†", "ğŸ˜"]

    if emoji in bad_emojis:
        is_negative = True
    if emoji in good_emojis and not is_negative:
        is_negative = False

    # ì¼€ì´ìŠ¤ ë¶„ê¸°
    if is_body:
        return (
            "ì˜¤ëŠ˜ì€ ëª¸ì´ ì¡°ê¸ˆ ë¬´ê±°ìš´ ë‚  ê°™ì•„ìš”. ë”°ëœ»í•œ ë¬¼ ë§ì´ ë§ˆì‹œê³ , "
            "ë¬´ë¦¬í•˜ì§€ ë§ê³  ì¼ì° ì‰¬ì–´ ì£¼ë©´ ì¢‹ê² ì–´ìš”."
        )

    if is_negative and is_stress:
        return (
            "ìš”ì¦˜ í•  ì¼ì´ ë§ì•„ì„œ ë§ˆìŒì´ ê½¤ ì§€ì¹œ ìƒíƒœì¸ ê²ƒ ê°™ì•„ìš”. "
            "ì˜¤ëŠ˜ í•´ì•¼ í•  ê²ƒ ì¤‘ì—ì„œ ê¼­ ì¤‘ìš”í•œ ê²ƒ í•œë‘ ê°œë§Œ ì •ë¦¬í•˜ê³ , "
            "ì ê¹ ì‚°ì±…ì´ë‚˜ ìŠ¤íŠ¸ë ˆì¹­ìœ¼ë¡œ ë¨¸ë¦¬ë¥¼ ì‹í˜€ë³´ë©´ ì–´ë–¨ê¹Œìš”?"
        )

    if is_negative:
        return (
            "ê¸°ë¶„ì´ ì¡°ê¸ˆ ì•„ë˜ìª½ìœ¼ë¡œ ë‚´ë ¤ê°€ ìˆëŠ” í•˜ë£¨ ê°™ì•„ìš”. "
            "ìŠ¤ìŠ¤ë¡œë¥¼ ëª°ì•„ë¶™ì´ê¸°ë³´ë‹¤ëŠ”, ì¢‹ì•„í•˜ëŠ” ìŒì•…ì„ í‹€ì–´ë†“ê³  "
            "ì§§ê²Œë¼ë„ íœ´ì‹ ì‹œê°„ì„ ë§Œë“¤ì–´ ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        )

    if is_stress:
        return (
            "í•´ì•¼ í•  ì¼ë“¤ì´ ë¨¸ë¦¿ì†ì—ì„œ ë¹™ê¸€ë¹™ê¸€ ë„ëŠ” ëŠë‚Œì¼ ìˆ˜ ìˆì–´ìš”. "
            "ê°„ë‹¨í•œ í•  ì¼ ëª©ë¡ì„ ì ì–´ë‘ê³ , ê°€ì¥ ì‘ì€ ê²ƒ í•˜ë‚˜ë¶€í„° "
            "ì°¨ê·¼ì°¨ê·¼ ì •ë¦¬í•´ë³´ë©´ ë§ˆìŒì´ í›¨ì”¬ ê°€ë²¼ì›Œì§ˆ ê±°ì˜ˆìš”."
        )

    # ê¸°ë³¸(ë¬´ë‚œí•œ ë‚ )
    return (
        "ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì´ë ‡ê²Œ ê¸°ë¡í•œ ê²ƒë§Œìœ¼ë¡œë„ ì´ë¯¸ ì˜ í•˜ê³  ìˆì–´ìš”. "
        "ì§€ê¸ˆ ëŠë‚Œì„ ì ê¹ ë” ëŒì•„ë³´ê³ , ë‚¨ì€ ì‹œê°„ì—ëŠ” ë‚˜ë¥¼ ìœ„í•œ ì‘ì€ ë³´ìƒì„ ì¤€ë¹„í•´ë³´ë©´ ì–´ë–¨ê¹Œìš”?"
    )


@app.post("/advice")
async def generate_advice(payload: AdviceRequest):
    """
    ì˜¤ëŠ˜ ê°ì •/ë©”ëª¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§§ì€ ìê¸°ê´€ë¦¬ í”¼ë“œë°±ì„ ìƒì„±í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸.
    - input: text(í•„ìˆ˜), emoji(ì„ íƒ)
    - output: í•œê¸€ ì¡°ì–¸ 2~3ë¬¸ì¥
    """
    if not payload.text.strip():
        raise HTTPException(status_code=400, detail="textëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    user_text = payload.text.strip()

    # OpenAIë¥¼ ë¨¼ì € ì‹œë„í•˜ê³ , ì‹¤íŒ¨í•˜ë©´ ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë¬¸êµ¬ë¡œ ëŒ€ì²´
    try:
        emoji_part = (
            f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ê°ì • ì´ëª¨ì§€ëŠ” '{payload.emoji}'ì…ë‹ˆë‹¤.\n"
            if payload.emoji
            else ""
        )

        system_prompt = """
ë„ˆëŠ” ì‚¬ìš©ìì˜ í•˜ë£¨ ê°ì •ê³¼ ë©”ëª¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§§ì€ ìê¸°ê´€ë¦¬ í”¼ë“œë°±ì„ ì œì•ˆí•˜ëŠ” ì½”ì¹˜ì•¼.

ê·œì¹™:
- ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  í¸ì•ˆí•˜ê²Œ, ë°˜ë§/ì¡´ëŒ“ë§ í˜¼ìš© ì—†ì´ "~ìš”"ì²´ë¡œ í†µì¼.
- 2~3ë¬¸ì¥ ì •ë„ë¡œ ì§§ê²Œ.
- ë„ˆë¬´ ê±°ì°½í•œ ëª©í‘œ ë§ê³ , ì˜¤ëŠ˜ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” ê°€ë²¼ìš´ í–‰ë™ì„ ì œì•ˆí•´ì¤˜.
- ì‚¬ìš©ìë¥¼ í‰ê°€í•˜ê±°ë‚˜ ë¹„ë‚œí•˜ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆ.
- ì¶œë ¥ í˜•ì‹ì€ ì¤„ë°”ê¿ˆ í¬í•¨ ììœ ë¡­ê²Œ, ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ëŠ” ì“°ì§€ ë§ ê²ƒ.
"""

        completion = openai_client.responses.create(
            model=ADVICE_MODEL,
            input=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"""{emoji_part}ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì˜¤ëŠ˜ ê°ì • ë©”ëª¨ì…ë‹ˆë‹¤:

\"\"\"{user_text}\"\"\"""",
                },
            ],
            max_output_tokens=120,
        )

        advice_text = completion.output[0].content[0].text.strip()

        return {
            "advice": advice_text,
            "model": ADVICE_MODEL,
            "source": "openai",
        }

    except Exception as e:
        # í¬ë ˆë”§/ë„¤íŠ¸ì›Œí¬/ê¸°íƒ€ ì˜¤ë¥˜ â†’ ë¡œì»¬ ì¡°ì–¸ìœ¼ë¡œ ëŒ€ì²´
        print("âŒ /advice ìƒì„± ì˜¤ë¥˜ (fallback ì‚¬ìš©):", e)
        fallback = build_fallback_advice(user_text, payload.emoji)

        return {
            "advice": fallback,
            "model": "local-fallback",
            "source": "fallback",
            "note": "OpenAI API ì¿¼í„°/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë¡œ ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ì¡°ì–¸ì„ ë°˜í™˜í–ˆì–´ìš”.",
        }

# â”€â”€ í—¬ìŠ¤ì²´í¬
@app.get("/health")
def health():
    return {"status": "ok", "model": MODEL_NAME, "version": VERSION, "device": str(DEVICE)}

# â”€â”€ ê°ì • ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze")
def analyze_sentiment(input: TextInput):
    enc = tokenizer(
        input.text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    ).to(DEVICE)

    with torch.no_grad():
        logits = model(**enc).logits

    probs = F.softmax(logits, dim=-1)[0].cpu()  # [neg, neu, pos]
    p_neg, p_neu, p_pos = map(float, probs)

    label = max(
        {"positive": p_pos, "neutral": p_neu, "negative": p_neg},
        key=lambda k: {"positive": p_pos, "neutral": p_neu, "negative": p_neg}[k]
    )

    return {
        "text": input.text,
        "positive": round(p_pos, 3),
        "neutral": round(p_neu, 3),
        "negative": round(p_neg, 3),
        "label": label,
        "device": str(DEVICE),
    }

# â”€â”€ Quantum ê°ì • íŒŒë™ ì‹œë®¬ë ˆì´ì…˜
# NOTE: ì´ˆê¸° ë””ë²„ê¹…ì„ ìœ„í•´ response_model ê²€ì¦ì„ ì ì‹œ ë•ë‹ˆë‹¤. (ì—ëŸ¬ detail ë°”ë¡œ ë³´ê¸° ìœ„í•¨)
#       ë¬¸ì œ ì—†ì´ ë™ì‘ í™•ì¸ í›„ ì•„ë˜ ë°ì½”ë ˆì´í„°ë¥¼ response_model=QuantumSimResponse ë¡œ ë°”ê¿”ë„ ë©ë‹ˆë‹¤.
@app.post("/quantum/simulate")  # , response_model=QuantumSimResponse
def quantum_simulate(req: QuantumSimRequest):
    try:
        # 1) ì…ë ¥ ì†ŒìŠ¤ ê²°ì •
        if req.probs is None and (req.text is None or not str(req.text).strip()):
            raise ValueError("Either text or probs must be provided.")

        if req.probs is None:
            # í…ìŠ¤íŠ¸ â†’ ëª¨ë¸ í™•ë¥ 
            enc = tokenizer(
                req.text, return_tensors="pt", truncation=True, padding=True, max_length=256
            ).to(DEVICE)
            with torch.no_grad():
                logits = model(**enc).logits
            probs_model = F.softmax(logits, dim=-1)[0].cpu().numpy()  # [neg, neu, pos]
            p_neg, p_neu, p_pos = float(probs_model[0]), float(probs_model[1]), float(probs_model[2])
        else:
            # í™•ë¥  ì§ì ‘ ì…ë ¥
            p_pos = float(req.probs.positive)
            p_neu = float(req.probs.neutral)
            p_neg = float(req.probs.negative)

        # 2) ì •ê·œí™” ë° ê²€ì¦
        probs = np.array([p_pos, p_neu, p_neg], dtype=float)
        s = float(probs.sum())
        if not np.isfinite(s) or s <= 0:
            raise ValueError(f"Invalid probabilities: {probs.tolist()}")
        probs = probs / s  # í•© 1

        # 3) íŒŒë¼ë¯¸í„° (ì „ë¶€ float ê°•ì œ)
        labels = ["positive", "neutral", "negative"]
        phases = np.array(
            [float(req.phases.positive), float(req.phases.neutral), float(req.phases.negative)],
            dtype=float,
        )
        omegas = np.array(
            [float(req.omegas.positive), float(req.omegas.neutral), float(req.omegas.negative)],
            dtype=float,
        )
        amps = np.sqrt(probs.astype(float))
        c = float(req.coherence)
        if not (0.0 <= c <= 1.0):
            raise ValueError(f"coherence must be in [0,1], got {c}")

        duration = float(req.duration)
        dt = float(req.dt)
        if dt <= 0 or duration <= 0:
            raise ValueError(f"duration/dt must be > 0, got duration={duration}, dt={dt}")

        # 4) ì‹œê°„ì¶•
        t = np.arange(0.0, duration + 1e-12, dt, dtype=float)
        base = float(np.sum(amps**2))
        I_total = np.full(t.shape, base, dtype=float)

        # 5) ê°„ì„­í•­
        for i in range(3):
            for j in range(i + 1, 3):
                d_omega = float(omegas[i] - omegas[j])
                d_phase = float(phases[i] - phases[j])
                I_total += 2.0 * c * float(amps[i] * amps[j]) * np.cos(d_omega * t + d_phase)

        # 6) ìš”ì•½
        summary = {
            "mean": float(np.mean(I_total)),
            "max": float(np.max(I_total)),
            "min": float(np.min(I_total)),
            "ptp": float(np.ptp(I_total)),
        }

        components = [
            {
                "label": labels[k],
                "probability": float(probs[k]),
                "amplitude": float(amps[k]),
                "phase": float(phases[k]),
                "omega": float(omegas[k]),
            }
            for k in range(3)
        ]

        return {
            "time": t.tolist(),
            "I_total": I_total.tolist(),
            "components": components,
            "summary": summary,
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))